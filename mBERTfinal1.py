# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18IP7UvHG5ivo2CyNfSihY-LT8rxPrzzL
"""

from google.colab import files
uploaded = files.upload()

import torch
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
import warnings
warnings.filterwarnings('ignore')



print("="*80)
print(" GENERATING MULTILINGUAL SENTIMENT DATASETS")
print("="*80)

# Hindi Dataset
hindi_data = {
    "Review": [
        # Negative reviews
        "यह उत्पाद बिल्कुल खराब है", "गुणवत्ता बहुत खराब है", "पैसे की बर्बादी है",
        "बिल्कुल काम नहीं करता", "बहुत निराशाजनक अनुभव", "खराब सेवा मिली",
        "गलत उत्पाद भेजा गया", "समय की बर्बादी है यह", "बिल्कुल संतुष्ट नहीं हूं",
        "बुरा अनुभव रहा", "ये उत्पाद मत खरीदें", "बेकार चीज़ है",

        # Neutral reviews
        "यह ठीक ठाक उत्पाद है", "साधारण गुणवत्ता है", "कुछ खास नहीं है",
        "औसत उत्पाद है यह", "न अच्छा न बुरा", "मिला जुला अनुभव रहा",
        "उम्मीद के अनुसार है", "साधारण सेवा मिली", "कीमत के हिसाब से ठीक है",
        "कुछ सुधार की जरूरत है", "औसत दर्जे का है", "सामान्य उत्पाद है",

        # Positive reviews
        "बहुत अच्छा उत्पाद है", "उत्कृष्ट गुणवत्ता है", "बेहतरीन सेवा मिली",
        "शानदार अनुभव रहा", "पैसे वसूल उत्पाद है", "बहुत संतुष्ट हूं",
        "शानदार गुणवत्ता है", "उम्मीद से बेहतर है", "जरूर खरीदें यह",
        "कमाल का उत्पाद है", "बहुत ही बढ़िया है", "लाजवाब सेवा मिली"
    ],
    "Summary": [
        "खराब", "बुरा", "निराशाजनक", "बेकार", "असंतोष", "गलत",
        "समय बर्बाद", "नाखुश", "संतुष्ट नहीं", "बुरा", "मत लें", "बेकार",
        "ठीक", "साधारण", "औसत", "मध्यम", "सामान्य", "मिला जुला",
        "उम्मीद अनुसार", "सामान्य", "ठीक है", "सुधार चाहिए", "औसत", "सामान्य",
        "उत्कृष्ट", "बढ़िया", "शानदार", "लाजवाब", "पैसे वसूल", "संतुष्ट",
        "उत्तम", "बेहतरीन", "जरूर लें", "कमाल", "बहुत अच्छा", "शानदार"
    ],
    "Sentiment": ["negative"]*12 + ["neutral"]*12 + ["positive"]*12
}

# Bengali Dataset
bengali_data = {
    "Review": [
        # Negative
        "এই পণ্যটি খুবই খারাপ", "গুণমান ভালো নয়", "টাকার অপচয়",
        "একেবারেই কাজ করে না", "খুব হতাশাজনক", "খারাপ সেবা পেয়েছি",
        "ভুল পণ্য পাঠানো হয়েছে", "সময়ের অপচয়", "সন্তুষ্ট নই",
        "খারাপ অভিজ্ঞতা", "কিনবেন না এটা", "অকেজো জিনিস",

        # Neutral
        "এটা ঠিক আছে", "সাধারণ মানের", "কিছু বিশেষ নয়",
        "গড় মানের পণ্য", "ভালো না খারাপও না", "মিশ্র অভিজ্ঞতা",
        "প্রত্যাশা অনুযায়ী", "সাধারণ সেবা", "দামের জন্য ঠিক আছে",
        "উন্নতি প্রয়োজন", "গড় মানের", "সাধারণ পণ্য",

        # Positive
        "খুব ভালো পণ্য", "চমৎকার গুণমান", "দারুণ সেবা পেয়েছি",
        "দুর্দান্ত অভিজ্ঞতা", "টাকার মূল্য আছে", "খুবই সন্তুষ্ট",
        "চমৎকার মান", "প্রত্যাশার চেয়ে ভালো", "অবশ্যই কিনুন",
        "অসাধারণ পণ্য", "খুবই ভালো", "দুর্দান্ত সেবা"
    ],
    "Summary": [
        "খারাপ", "বাজে", "হতাশাজনক", "অকেজো", "অসন্তোষ", "ভুল",
        "সময় নষ্ট", "অসন্তুষ্ট", "খারাপ", "বাজে", "নেবেন না", "অকেজো",
        "ঠিক আছে", "সাধারণ", "গড়", "মাঝারি", "সাধারণ", "মিশ্র",
        "প্রত্যাশা মতো", "সাধারণ", "ঠিক", "উন্নতি চাই", "গড়", "সাধারণ",
        "উৎকৃষ্ট", "ভালো", "চমৎকার", "দুর্দান্ত", "মূল্যবান", "সন্তুষ্ট",
        "উত্তম", "দারুণ", "নিন", "অসাধারণ", "খুব ভালো", "চমৎকার"
    ],
    "Sentiment": ["negative"]*12 + ["neutral"]*12 + ["positive"]*12
}

# Tamil Dataset
tamil_data = {
    "Review": [
        # Negative
        "இந்த பொருள் மிகவும் மோசமானது", "தரம் மோசமாக உள்ளது", "பணம் வீணானது",
        "வேலை செய்யவில்லை", "மிகவும் ஏமாற்றமளிக்கிறது", "மோசமான சேவை",
        "தவறான பொருள் அனுப்பப்பட்டது", "நேரம் வீணானது", "திருப்தியில்லை",
        "மோசமான அனுபவம்", "வாங்க வேண்டாம்", "பயனற்றது",

        # Neutral
        "இது சரியாக உள்ளது", "சாதாரண தரம்", "சிறப்பு எதுவும் இல்லை",
        "சராசரி பொருள்", "நல்லதும் இல்லை கெட்டதும் இல்லை", "கலவையான அனுபவம்",
        "எதிர்பார்ப்புக்கு ஏற்ப", "சாதாரண சேவை", "விலைக்கு ஏற்றது",
        "முன்னேற்றம் தேவை", "சராசரி", "சாதாரண பொருள்",

        # Positive
        "மிகவும் நல்ல பொருள்", "சிறந்த தரம்", "அருமையான சேவை",
        "அற்புதமான அனுபவம்", "பணத்திற்கு மதிப்பு உண்டு", "மிகவும் திருப்தி",
        "அருமையான தரம்", "எதிர்பார்த்ததை விட சிறப்பு", "கண்டிப்பாக வாங்கவும்",
        "அற்புதமான பொருள்", "மிகவும் நல்லது", "சிறந்த சேவை"
    ],
    "Summary": [
        "மோசம்", "கெட்டது", "ஏமாற்றம்", "பயனற்றது", "அதிருப்தி", "தவறு",
        "நேர விரயம்", "அதிருப்தி", "மோசம்", "கெட்டது", "வேண்டாம்", "பயனற்றது",
        "சரி", "சாதாரணம்", "சராசரி", "நடுத்தரம்", "சாதாரணம்", "கலவை",
        "எதிர்பார்ப்பு", "சாதாரணம்", "சரி", "முன்னேற்றம்", "சராசரி", "சாதாரணம்",
        "சிறப்பு", "நல்லது", "அருமை", "அற்புதம்", "மதிப்பு", "திருப்தி",
        "உயர்ந்தது", "சிறந்தது", "வாங்கவும்", "அற்புதம்", "மிக நல்லது", "அருமை"
    ],
    "Sentiment": ["negative"]*12 + ["neutral"]*12 + ["positive"]*12
}

# Telugu Dataset
telugu_data = {
    "Review": [
        # Negative
        "ఈ ఉత్పత్తి చాలా చెడ్డది", "నాణ్యత చెడ్డగా ఉంది", "డబ్బు వృథా",
        "పని చేయడం లేదు", "చాలా నిరాశపరిచింది", "చెడ్డ సేవ",
        "తప్పు ఉత్పత్తి పంపించారు", "సమయం వృథా", "సంతృప్తి లేదు",
        "చెడ్డ అనుభవం", "కొనవద్దు", "పనికిరాదు",

        # Neutral
        "ఇది బాగానే ఉంది", "సాధారణ నాణ్యత", "ప్రత్యేకత ఏమీ లేదు",
        "సగటు ఉత్పత్తి", "మంచిది కాదు చెడ్డది కాదు", "మిశ్రమ అనుభవం",
        "ఆశించినట్లుగా", "సాధారణ సేవ", "ధరకు తగినది",
        "మెరుగుదల అవసరం", "సగటు", "సాధారణ ఉత్పత్తి",

        # Positive
        "చాలా మంచి ఉత్పత్తి", "అద్భుతమైన నాణ్యత", "అద్భుతమైన సేవ",
        "గొప్ప అనుభవం", "డబ్బుకు విలువ ఉంది", "చాలా సంతృప్తి",
        "అద్భుతమైన నాణ్యత", "ఊహించిన దానికంటే మంచిది", "తప్పకుండా కొనండి",
        "అద్భుతమైన ఉత్పత్తి", "చాలా బాగుంది", "అద్భుతమైన సేవ"
    ],
    "Summary": [
        "చెడ్డది", "బాగాలేదు", "నిరాశ", "పనికిరాదు", "అసంతృప్తి", "తప్పు",
        "సమయం వృథా", "అసంతృప్తి", "చెడ్డది", "బాగాలేదు", "వద్దు", "పనికిరాదు",
        "బాగుంది", "సాధారణం", "సగటు", "మధ్యస్థం", "సాధారణం", "మిశ్రమం",
        "ఆశించినట్లు", "సాధారణం", "బాగుంది", "మెరుగు", "సగటు", "సాధారణం",
        "అద్భుతం", "మంచిది", "గొప్పది", "అద్భుతం", "విలువ", "సంతృప్తి",
        "ఉత్తమం", "గొప్పది", "కొనండి", "అద్భుతం", "చాలా మంచిది", "అద్భుతం"
    ],
    "Sentiment": ["negative"]*12 + ["neutral"]*12 + ["positive"]*12
}

# Marathi Dataset
marathi_data = {
    "Review": [
        # Negative
        "हा उत्पादन अगदी वाईट आहे", "गुणवत्ता खराब आहे", "पैशांचा अपव्यय",
        "काम करत नाही", "खूप निराशाजनक", "वाईट सेवा मिळाली",
        "चुकीचा माल पाठवला", "वेळेचा अपव्यय", "समाधानी नाही",
        "वाईट अनुभव", "विकत घेऊ नका", "निरुपयोगी आहे",

        # Neutral
        "हे ठीक आहे", "सामान्य गुणवत्ता", "काही खास नाही",
        "सरासरी उत्पादन", "चांगले नाही वाईटही नाही", "मिश्र अनुभव",
        "अपेक्षेप्रमाणे", "सामान्य सेवा", "किंमतीनुसार योग्य",
        "सुधारणा आवश्यक", "सरासरी", "सामान्य उत्पादन",

        # Positive
        "खूप चांगला उत्पादन", "उत्तम गुणवत्ता", "उत्कृष्ट सेवा",
        "अप्रतिम अनुभव", "पैशांची किंमत आहे", "खूप समाधानी",
        "उत्कृष्ट गुणवत्ता", "अपेक्षेपेक्षा चांगले", "नक्की विकत घ्या",
        "छान उत्पादन", "खूप चांगले", "उत्कृष्ट सेवा"
    ],
    "Summary": [
        "वाईट", "खराब", "निराशा", "निरुपयोगी", "असमाधान", "चुकीचे",
        "वेळ वाया", "असमाधानी", "वाईट", "खराब", "घेऊ नका", "निरुपयोगी",
        "ठीक", "सामान्य", "सरासरी", "मध्यम", "सामान्य", "मिश्र",
        "अपेक्षित", "सामान्य", "ठीक", "सुधार", "सरासरी", "सामान्य",
        "उत्कृष्ट", "चांगले", "छान", "अप्रतिम", "किंमत", "समाधानी",
        "उत्तम", "उत्कृष्ट", "घ्या", "छान", "खूप चांगले", "उत्कृष्ट"
    ],
    "Sentiment": ["negative"]*12 + ["neutral"]*12 + ["positive"]*12
}


# DataFrames
hindi_df = pd.DataFrame(hindi_data)
bengali_df = pd.DataFrame(bengali_data)
tamil_df = pd.DataFrame(tamil_data)
telugu_df = pd.DataFrame(telugu_data)
marathi_df = pd.DataFrame(marathi_data)

# commands for saving the data in csv format
hindi_df.to_csv("hindi_sentiment.csv", index=False, encoding='utf-8')
bengali_df.to_csv("bengali_sentiment.csv", index=False, encoding='utf-8')
tamil_df.to_csv("tamil_sentiment.csv", index=False, encoding='utf-8')
telugu_df.to_csv("telugu_sentiment.csv", index=False, encoding='utf-8')
marathi_df.to_csv("marathi_sentiment.csv", index=False, encoding='utf-8')

print(" Generated Hindi dataset: 36 samples")
print(" Generated Bengali dataset: 36 samples")
print(" Generated Tamil dataset: 36 samples")
print(" Generated Telugu dataset: 36 samples")
print(" Generated Marathi dataset: 36 samples")
print("="*80)



print("\nLoading all datasets...")

# Load English dataset
df_english = pd.read_csv("Equal.csv")
df_english["text"] = df_english["Review"].astype(str) + " " + df_english["Summary"].astype(str)
df_english["Sentiment"] = df_english["Sentiment"].str.lower().str.strip()

# Load Indian language datasets
df_hindi = pd.read_csv("hindi_sentiment.csv")
df_hindi["text"] = df_hindi["Review"].astype(str) + " " + df_hindi["Summary"].astype(str)
df_hindi["Sentiment"] = df_hindi["Sentiment"].str.lower().str.strip()

df_bengali = pd.read_csv("bengali_sentiment.csv")
df_bengali["text"] = df_bengali["Review"].astype(str) + " " + df_bengali["Summary"].astype(str)
df_bengali["Sentiment"] = df_bengali["Sentiment"].str.lower().str.strip()

df_tamil = pd.read_csv("tamil_sentiment.csv")
df_tamil["text"] = df_tamil["Review"].astype(str) + " " + df_tamil["Summary"].astype(str)
df_tamil["Sentiment"] = df_tamil["Sentiment"].str.lower().str.strip()

df_telugu = pd.read_csv("telugu_sentiment.csv")
df_telugu["text"] = df_telugu["Review"].astype(str) + " " + df_telugu["Summary"].astype(str)
df_telugu["Sentiment"] = df_telugu["Sentiment"].str.lower().str.strip()

df_marathi = pd.read_csv("marathi_sentiment.csv")
df_marathi["text"] = df_marathi["Review"].astype(str) + " " + df_marathi["Summary"].astype(str)
df_marathi["Sentiment"] = df_marathi["Sentiment"].str.lower().str.strip()

# Label mapping for Reviews
label_map = {
    "negative": 0,
    "neutral": 1,
    "positive": 2
}

# This portion apply mapping to all the datasets
for df in [df_english, df_hindi, df_bengali, df_tamil, df_telugu, df_marathi]:
    df_clean = df[df["Sentiment"].isin(label_map.keys())]
    df["label"] = df["Sentiment"].map(label_map)

# Filtering valid labels
df_english = df_english[df_english["Sentiment"].isin(label_map.keys())]
df_hindi = df_hindi[df_hindi["Sentiment"].isin(label_map.keys())]
df_bengali = df_bengali[df_bengali["Sentiment"].isin(label_map.keys())]
df_tamil = df_tamil[df_tamil["Sentiment"].isin(label_map.keys())]
df_telugu = df_telugu[df_telugu["Sentiment"].isin(label_map.keys())]
df_marathi = df_marathi[df_marathi["Sentiment"].isin(label_map.keys())]

print(f"\n English: {len(df_english)} samples")
print(f" Hindi: {len(df_hindi)} samples")
print(f" Bengali: {len(df_bengali)} samples")
print(f" Tamil: {len(df_tamil)} samples")
print(f" Telugu: {len(df_telugu)} samples")
print(f" Marathi: {len(df_marathi)} samples")

# Combine all datasets
df_combined = pd.concat([df_english, df_hindi, df_bengali, df_tamil, df_telugu, df_marathi], ignore_index=True)

print(f"\nTotal combined samples: {len(df_combined)}")
print("\nLabel distribution:")
print(df_combined["label"].value_counts().sort_index())
print("="*80)

#For the Train/Val split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_combined["text"].tolist(),
    df_combined["label"].tolist(),
    test_size=0.2,
    stratify=df_combined["label"],
    random_state=42
)

print(f"\nTrain samples: {len(train_texts)}")
print(f"Val samples: {len(val_texts)}")


#Model Training

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

MODEL_NAME = "bert-base-multilingual-cased"

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

model = BertForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3
)

model.to(device)

train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding=True,
    max_length=128
)

val_encodings = tokenizer(
    val_texts,
    truncation=True,
    padding=True,
    max_length=128
)


class ReviewDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(int(self.labels[idx]))
        return item

    def __len__(self):
        return len(self.labels)


train_dataset = ReviewDataset(train_encodings, train_labels)
val_dataset = ReviewDataset(val_encodings, val_labels)

training_args = TrainingArguments(
    output_dir="./results_multilingual",
    num_train_epochs=2,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs_multilingual",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    warmup_steps=100,
    weight_decay=0.01,
    fp16=torch.cuda.is_available(),
    report_to="none"
)



def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)

    # Calculate precision, recall, f1 for each class and average
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='weighted'
    )

    accuracy = accuracy_score(labels, preds)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

print("\n" + "="*80)
print("TRAINING MULTILINGUAL MODEL")
print("="*80)
trainer.train()


#For Saving the model

model.save_pretrained("mbert_multilingual_sentiment20")
tokenizer.save_pretrained("mbert_multilingual_sentiment20")
print("\nModel saved to 'mbert_multilingual_sentiment20/'")


print("\n" + "="*80)
print("DETAILED VALIDATION EVALUATION")
print("="*80)

model.eval()

# Get predictions on validation set
val_inputs = tokenizer(
    val_texts,
    truncation=True,
    padding=True,
    max_length=128,
    return_tensors="pt"
).to(device)

with torch.no_grad():
    outputs = model(**val_inputs)
    predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()

# Calculate all metrics
accuracy = accuracy_score(val_labels, predictions)
precision, recall, f1, support = precision_recall_fscore_support(
    val_labels, predictions, average='weighted'
)

# Calculate per-class metrics
precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(
    val_labels, predictions, average=None
)

print("\nOVERALL VALIDATION METRICS:")
print("-" * 80)
print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")

print("\nPER-CLASS METRICS:")
print("-" * 80)
label_names = ["Negative", "Neutral", "Positive"]
print(f"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}")
print("-" * 80)
for i, label_name in enumerate(label_names):
    print(f"{label_name:<12} {precision_per_class[i]:<12.4f} {recall_per_class[i]:<12.4f} "
          f"{f1_per_class[i]:<12.4f} {support_per_class[i]:<10}")

print("\nDETAILED CLASSIFICATION REPORT:")
print("-" * 80)
print(classification_report(val_labels, predictions, target_names=label_names))
print("="*80)


# Prediction function with updated label mapping
label_reverse_map = {
    0: "Negative",
    1: "Neutral",
    2: "Positive"
}

def predict_review(text, language=None):
    """Predict sentiment for multilingual text"""
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    probs = torch.softmax(outputs.logits, dim=1)
    prediction = torch.argmax(probs, dim=1).item()
    confidence = probs.max().item() * 100

    print("\n" + "="*60)
    if language:
        print(f"Language: {language}")
    print(f"Text: {text}")
    print("-"*60)
    print(f"Prediction: {label_reverse_map[prediction]}")
    print(f"Confidence: {confidence:.2f}%")
    print(f"Probabilities: Neg={probs[0][0].item()*100:.1f}% | Neu={probs[0][1].item()*100:.1f}% | Pos={probs[0][2].item()*100:.1f}%")
    print("="*60)


# Example predictions
print("\n" + "="*80)
print("SAMPLE PREDICTIONS")
print("="*80)

# Test with different languages
test_samples = [
    ("यह उत्पाद बहुत अच्छा है", "Hindi"),
    ("এই পণ্যটি খুবই খারাপ", "Bengali"),
    ("This product is amazing", "English"),
]

for text, lang in test_samples:
    predict_review(text, lang)